{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c2662a3",
   "metadata": {},
   "source": [
    "# Session 1: QuotationTool\n",
    "In this notebook, you will use the *QuotationTool* to extract quotes from a list of texts. In addition to extracting the quotes, the tool also provides information about who the speakers are, the location of the quotes (and the speakers) within the text, the identified named entities, etc., which can be useful for your text analysis.  \n",
    "\n",
    "**Note:** This code has been adapted (with permission) from the [GenderGapTracker GitHub page](https://github.com/sfu-discourse-lab/GenderGapTracker/tree/master/nlp/english) and modified to run on a Jupyter Notebook. The quotation tool’s accuracy rate is evaluated in [this article](https://doi.org/10.1371/journal.pone.0245533).\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>User guide to using a Jupyter Notebook</b> \n",
    "\n",
    "If you are new to Jupyter Notebook, feel free to take a quick look at [this user guide](https://github.com/Australian-Text-Analytics-Platform/quotation-tool/blob/main/documents/jupyter-notebook-guide.pdf) for basic information on how to use a notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc99ca86",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "Before you begin, you need to import the QuotationTool and the necessary libraries and initiate them to run in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6720419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the QuotationTool\n",
    "from extract_display_quotes import QuotationTool, DownloadFileLink\n",
    "\n",
    "# initialize the QuotationTool\n",
    "qt = QuotationTool()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a9cb8a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Installing Libraries</b> \n",
    "\n",
    "The requirements file <b>environment.yml</b> is included with this notebook. Take a look inside to find out what libraries you have just installed with the above command.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf517c9",
   "metadata": {},
   "source": [
    "## 2. Load the data\n",
    "This notebook will allow you to extract quotes directly from a text file (or a number of text files). Alternatively, you can also extract quotes from a text column inside your excel spreadsheet ([see an example here](https://github.com/Australian-Text-Analytics-Platform/quotation-tool/blob/main/documents/sample_texts.xlsx?raw=true)).  \n",
    "\n",
    "<table style='margin-left: 10px'><tr>\n",
    "<td> <img src='./img/txt_icon.png' style='width: 45px'/> </td>\n",
    "<td> <img src='./img/xlsx_icon.png' style='width: 55px'/> </td>\n",
    "<td> <img src='./img/csv_icon.png' style='width: 45px'/> </td>\n",
    "<td> <img src='./img/zip_icon.png' style='width: 45px'/> </td>\n",
    "</tr></table>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Uploading your text files</b> \n",
    "    \n",
    "If you have a large number of text files (more than 10MB in total), we suggest you compress (zip) them and upload the zip file instead. If you need assistance on how to compress your file, please check [the user guide](https://github.com/Australian-Text-Analytics-Platform/quotation-tool/blob/main/documents/jupyter-notebook-guide.pdf) for more info. \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Large file upload</b> \n",
    "    \n",
    "If you have ongoing issues with the file upload, please re-launch the notebook via Binder again. If the issue persists, consider restarting your computer.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad98bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the text files and/or excel spreadsheets onto the system\n",
    "display(qt.upload_box)\n",
    "print('Uploading large files may take a while. Please be patient.')\n",
    "print('\\033[1mPlease wait and do not press any buttons until the progress bar appears...\\033[0m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c700ef8b",
   "metadata": {},
   "source": [
    "Once your files are uploaded, you can see a preview of the text in a table format (pandas dataframe).  \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tools:</b>    \n",
    "    \n",
    "- nltk: for sentence tokenization\n",
    "- spaCy: for text cleaning and normalisation\n",
    "- pandas: for storing and displaying in dataframe (table) format\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Specify the number of rows to display</b> \n",
    "    \n",
    "By default, you will preview the first 5 rows of the extracted quotes in a pandas dataframe (table) format. However, you can preview more or less rows by specifying the number of rows you wish to display in the variable 'n' below. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d35f71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the number of rows you wish to display\n",
    "n=5\n",
    "\n",
    "# display a preview of the pandas dataframe\n",
    "qt.text_df.head(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4039347d",
   "metadata": {},
   "source": [
    "## 3. Extract the quotes\n",
    "Once your texts have been stored in a pandas dataframe, you can begin to extract the quotes from the texts. You can also extract named entities from your text by setting the named entities you wish to include in the below *inc_ent* variable. If you are extracting quotes from a lot of texts, be patient. As a guideline, for a corpus with a file size of 54.13 MB (~26,000 newspaper articles in plain text format), it can take ca 45 minutes to extract quotes.    \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tools:</b>    \n",
    "\n",
    "- quote_extractor: for extracting quotes and speakers\n",
    "- spaCy: for extracting named entities\n",
    "    \n",
    "<b>Note:</b> this tool uses spaCy to tokenize the text, which initially splits the text into tokens based on whitespace characters, and then applies language specific rules to further refine the outcome. For example, the word “don’t” does not contain whitespace, but would be split into two tokens: “do” and “n’t”, whereas “U.K.” would remain as one token. For more information about spaCy tokenizer, please visit [this page](https://spacy.io/usage/linguistic-features#tokenization).\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Specify the number of rows to display</b> \n",
    "    \n",
    "By default, you will preview the first 5 rows of the extracted quotes in a pandas dataframe (table) format. However, you can preview more or less rows by specifying the number of rows you wish to display in the variable 'n' below. \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Memory limitation in Binder</b> \n",
    "    \n",
    "The free Binder deployment is only guaranteed a maximum of 2GB memory. Processing very large text files may cause the session (kernel) to re-start due to insufficient memory. Check [the user guide](https://github.com/Sydney-Informatics-Hub/HASS-29_Quotation_Tool/blob/main/documents/jupyter-notebook-guide.pdf) for more info. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab19f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the named entities you wish to include below\n",
    "inc_ent = ['ORG','PERSON','GPE','NORP','FAC','LOC']\n",
    "\n",
    "# specify the number of rows you wish to display\n",
    "n=5\n",
    "\n",
    "# extract quotes from the text and preview them in a pandas dataframe (table) format\n",
    "quotes_df = qt.get_quotes(inc_ent)\n",
    "\n",
    "# display a preview of the pandas dataframe\n",
    "quotes_df.head(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079bc8a7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>What information is included in the above table?</b> \n",
    "\n",
    "In general, the quotes are extracted either based on syntactic or heuristic rules. Some quotes can be stand-alone in a sentence, or followed by another quote (floating quote) from the same speaker. Please refer to [this document](https://doi.org/10.1371/journal.pone.0245533.s001) for further information about the quote extraction process.  \n",
    "    \n",
    "**text_id:** the unique ID of the text.\n",
    "    \n",
    "**text_name** the name of the text, i.e., the name of the .txt files or the 'text_name' column in the excel spreadsheet.\n",
    "    \n",
    "**quote_id/speaker_id:** the unique ID of the extracted quote/speaker.\n",
    "    \n",
    "**quote/speaker:** the content of the extracted quote and the speaker.\n",
    "    \n",
    "**verb:** the verb used to determine the extracted quote.\n",
    "    \n",
    "**quote_index/speaker_index/verb_index:** the location of the first and the last characters of the extracted quote/speaker/verb in the text.\n",
    "    \n",
    "**quote_entities/speaker_entities:** the entity name and type of the entities identified in the extracted quote/speaker.\n",
    "    \n",
    "**quote_token_count:** the length of the extracted quote (in character).\n",
    "    \n",
    "**quote_type:** the type of quote based on how it is extracted.\n",
    "    \n",
    "**floating_quote:** whether the extracted quote is a floating quote, i.e., a follow up quote from the same speaker (The value TRUE here means that the quote is a floating quote, while FALSE means that the quote is not a floating quote).\n",
    "\n",
    "**Quotation symbols:** Q (Quotation mark), S (Speaker), V (Verb), C (Content).  \n",
    "\n",
    "**Named Entities:**  PERSON (People, including fictional), NORP (Nationalities or religious or political groups), FAC (Buildings, airports, highways, etc.), ORG (Companies, agencies, institutions, etc.), GPE (Countries, cities, states), LOC (Non-GPE locations, mountain ranges, bodies of water).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70be6223",
   "metadata": {},
   "source": [
    "## 4. Display the quotes\n",
    "Once you have extracted the quotes, you can see a preview of the quotes using spaCy's visualisation tool, displaCy. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tools:</b>    \n",
    "\n",
    "- displaCy: for displaying quotes, speakers and named entities\n",
    "- ipywidgets: for interactive tool\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Select the text and the entities to show</b> \n",
    "\n",
    "In order to preview the extracted information, select the text you wish to analyse and which entities to show. Then, you can click the ***Preview*** button to display them and the ***Save Preview*** button to save them as an html file. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e536e55b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# display a preview of the extracted quotes, speakers and entities within the text\n",
    "qt.analyse_quotes(inc_ent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5f4dc4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Select the text and the entities to show</b> \n",
    "\n",
    "You can also display the top named entitites identified in the quotes and/or speakers. You just need to select the text to analyse (option to analyse 'all texts' is also available), whether to display the identified entities in the speakers and/or quotes, whether to display the entity names and/or types, the number of top entities to display and finally, click the ***Show Top Entities*** and ***Save Top Entities*** buttons to display and save them, respectively. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a92980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the top named entities identified in the quotes and/or speakers\n",
    "qt.analyse_entities(inc_ent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e56460",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Capitalized words</b> \n",
    "\n",
    "Please note that lowercase or UPPERCASE words such as quote, QUOTE, Quote, etc. are recognised as different words by the tool, so you may see that they are counted differently in the above analysis.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3607fd4b",
   "metadata": {},
   "source": [
    "## 5. Save the quotes\n",
    "Finally, you can run the below code to save the quotes pandas dataframe into an Excel spreadsheet and download them to your local computer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388ca926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify output directory and file name\n",
    "output_dir = './output/'\n",
    "file_name = 'quotes.xlsx'\n",
    "\n",
    "# save quotes_df into an Excel spreadsheet\n",
    "from pyexcelerate import Workbook\n",
    "values = [quotes_df.columns] + list(quotes_df.values)\n",
    "wb = Workbook()\n",
    "wb.new_sheet('Sheet1', data=values)\n",
    "wb.save(output_dir + file_name)\n",
    "\n",
    "# download quotes_df to your computer\n",
    "print('Click below to download:')\n",
    "display(DownloadFileLink(output_dir + file_name, 'quotes.xlsx'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f45e82d",
   "metadata": {},
   "source": [
    "# Session 2: Semantic Tagger (English)\n",
    "\n",
    "In this notebook, you will use the [Python Multilingual Ucrel Semantic Analysis System (PyMUSAS)](https://ucrel.github.io/pymusas/) to tag your text data so that you can extract token level semantic tags from the tagged text. PyMUSAS is a rule based token and Multi Word Expression (MWE) semantic tagger. The tagger can support any semantic tagset; however the currently released tagset is for the [Ucrel Semantic Analysis System (USAS)](https://ucrel.lancs.ac.uk/usas/) semantic tags. \n",
    "\n",
    "In addition to the USAS tags, you will also see the lemmas and Part-of-Speech (POS) tags in the text. For English, the tagger also identifies and tags Multi Word Expressions (MWE), i.e., expressions formed by two or more words that behave like a unit such as 'South Australia'. See further [this article](https://www.sciencedirect.com/science/article/abs/pii/S0885230805000112?via%3Dihub) for an evaluation on the tagger’s MWE capabilities for English.  \n",
    "\n",
    "\n",
    "**Note:** This code has been adapted (with permission) from the [PyMUSAS GitHub page](https://github.com/UCREL/pymusas) and modified to run on a Jupyter Notebook. PyMUSAS is an open-source project that has been created and funded by the [University Centre for Computer Corpus Research on Language (UCREL)](https://ucrel.lancs.ac.uk/) at [Lancaster University](https://www.lancaster.ac.uk/). For more information about PyMUSAS, please visit [the Usage Guides page](https://ucrel.github.io/pymusas/). An evaluation of the accuracy of the English semantic tagger is included in [this paper](https://www.lancaster.ac.uk/staff/rayson/publications/usas_lrec04ws.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cc65d7",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "Before you begin, you need to import the SemanticTagger and the necessary libraries and initiate them to run in this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08634a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the SemanticTagger\n",
    "print('Loading SemanticTagger...')\n",
    "from semantic_tagger_en import SemanticTagger, DownloadFileLink\n",
    "\n",
    "# initialize the SemanticTagger\n",
    "st = SemanticTagger()\n",
    "print('Finished loading.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797a1819",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Multi Word Expression (MWE)</b> \n",
    "    \n",
    "Below, you can choose to also identify and tag Multi Word Expressions (MWE), i.e., expressions formed by two or more words that behave like a unit such as 'South Australia'. However, please be aware that selecting the MWE option will make the extraction process much slower.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a2f522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select whether to include mwe extractions\n",
    "st.loading_tagger_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e992175",
   "metadata": {},
   "source": [
    "## 2. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845439fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy uploaded data\n",
    "st.text_df = qt.text_df.copy()\n",
    "\n",
    "# display uploaded text\n",
    "n=5\n",
    "\n",
    "st.text_df.head(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dcffcc",
   "metadata": {},
   "source": [
    "## 3. Add Semantic Tags\n",
    "Once your texts have been uploaded, you can begin to add semantic tags to the texts and download the results to your computer. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tools:</b>    \n",
    "\n",
    "- PyMUSAS RuleBasedTagger: for adding USAS token tags.\n",
    "- spaCy: for adding lemma and POS tags.\n",
    "    \n",
    "<b>Note:</b> this tool uses spaCy to tokenize the text, which initially splits the text into tokens based on whitespace characters, and then applies language specific rules to further refine the outcome. For example, the word “don’t” does not contain whitespace, but would be split into two tokens: “do” and “n’t”, whereas “U.K.” would remain as one token. For more information about spaCy tokenizer, please visit [this page](https://spacy.io/usage/linguistic-features#tokenization).\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Large corpus</b> \n",
    "    \n",
    "Whilst using this tool with a large corpus is possible, this tool is more appropriate for use with a smaller corpus (<1,000 text files). This is because adding semantic tags to each token in the texts may take a while, especially if MWE are also extracted from the texts. Be patient while the corpus is being semantically tagged. As a guideline, a corpus with a file size of 0.66 MB (~600 newspaper articles in plain text format) can take 6-7 minutes before tagging is completed when MWE is selected and about two minutes when MWE is not selected. \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95d9b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add semantic taggers to the uploaded texts\n",
    "print('Processing and adding semantic tags to your texts.')\n",
    "print('The counter will start soon. Please be patient...')\n",
    "st.tag_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5126b49c",
   "metadata": {},
   "source": [
    "Once you have tagged the texts, you can display them in the dataframe (table format) below. All you need to do is to select the tagged text you wish to display and click the 'Display tagged text' button. You can also filter the text to only display certain pos tags or usas tags only (multiple filter selections are possible). You can choose to display one semantically-tagged text file or compare two semantically-tagged text files with each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5940b312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display tagged text\n",
    "st.display_two_tag_texts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6133d47d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>What information is included in the above table?</b> \n",
    "\n",
    "**token:** each token in the sentence, e.g., word, punctuation, etc.\n",
    "    \n",
    "**lemma** the lemma of the token.\n",
    "    \n",
    "**pos:** part-of-speech tag of the token.\n",
    "    \n",
    "**start_index/end_index (mwe option only):** the start and end indices of the token(s).\n",
    "    \n",
    "**mwe (mwe option only):** whether the token is part of a multi-word expression.\n",
    "    \n",
    "**usas_tags:** the the Ucrel Semantic Analysis System (USAS) sematic tag of the token.\n",
    "    \n",
    "**usas_tags_def:** the definition of the USAS tag of the token.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Analyse the tagged text</b> \n",
    "\n",
    "You can also analyse the tagged texts using simple visualizations below. To do so, please select the text (including 'all texts') and the entity to analyse, and click the 'Show top entities' button. To check the top words in each entity (e.g., top USAS tag 'Personal names'), select the drop down options on the right (multiple selections possible) and click 'Show top words' to display. To save the displayed charts, click the 'Save analysis' button. Make sure you change n [number] in ‘Select n’ to display the top 5, 10, 15 etc entities or words. You can just use the visualisation on the left-hand side (if you analyse ‘all texts’ or one specific text/file) or you can use both visualisations to compare two texts/files with each other. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62989215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse tagged texts\n",
    "st.analyse_two_tags()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7ade23",
   "metadata": {},
   "source": [
    "## 4. Save tagged texts\n",
    "Finally, you can run the below code to save the tagged texts into a comma separated file (.csv) or an excel spreadsheet (.xlsx) containing the tagged texts, or a zip of pseudo-xml (.txt) tagged text files. You can then download them to your local computer and use them for further analysis if you wish.  \n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Windows users</b> \n",
    "\n",
    "For Windows users, we suggest to download the tagged texts in excel or pseudo-xml format to ensure the non-English characters are encoded properly (MacOS users can still download the tagged texts in csv format and use the Numbers app to read them). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd2db3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tagged texts\n",
    "st.save_options()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "50e9832c473c86713864f61257029e15f281e1af6f46dd8361ab2dc7ec7915dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
