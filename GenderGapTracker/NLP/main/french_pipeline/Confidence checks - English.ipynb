{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity checks for annotation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODING = 'utf-8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('AnnotationTable_byArticle_withReplacementArticles.xlsx')\n",
    "people_cols = [column for column in list(df.columns) if 'people' in column and len(column) > len('people')]\n",
    "source_cols = [column for column in list(df.columns) if 'sources' in column]\n",
    "json_files = ['OutputJsonFiles/'+f for f in os.listdir('OutputJsonFiles/') if f.endswith('.json')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excel Assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'articleID', 'outlet', 'articleBodyText', 'articleURL',\n",
       "       'authors', 'authorFemale', 'authorMale', 'authorUnknowns', 'people',\n",
       "       'peopleFemale', 'peopleMale', 'peopleUnknown', 'quotations',\n",
       "       'sourcesFemale', 'sourcesMale', 'sourcesUnknown', 'articleType',\n",
       "       'annotatorsComment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excelAssertions(row):\n",
    "\n",
    "    def customError(key, check, stacktrace=None):\n",
    "        if stacktrace is None:\n",
    "            stacktrace = []\n",
    "        \n",
    "        errorMsg = 'FAILED ASSERTION: '\n",
    "\n",
    "        if check is 'subset':\n",
    "            errorMsg += key + ' not subset of people'\n",
    "            stacktrace.append(key + ': ' + row[key])\n",
    "            stacktrace.append('people: ' + row['people'])\n",
    "        if check is 'superset':\n",
    "            errorMsg += 'people not superset of ' + key\n",
    "            stacktrace.append('people: ' + row['people'])\n",
    "            stacktrace.append(key + ': ' + row[key])\n",
    "        if check is 'eval':\n",
    "            errorMsg += 'Error evaluating ' + key + ' as list'\n",
    "        if check is 'disjoint':\n",
    "            errorMsg += key + ' not disjoint'\n",
    "\n",
    "        errorMsg += ', ' + row['articleID']\n",
    "        \n",
    "        if len(stacktrace) > 0:\n",
    "            for trace in stacktrace:\n",
    "                errorMsg += '\\n' + str(trace)\n",
    "        \n",
    "        return errorMsg + '\\n'\n",
    "    \n",
    "    excel_dict = {}\n",
    "    assertion_count = 0\n",
    "    assertion_errors = ''\n",
    "    \n",
    "    people = set()\n",
    "    try:\n",
    "        people = set(eval(row['people']))\n",
    "    except SyntaxError as e:\n",
    "        assertion_count += 1\n",
    "        assertion_errors += customError('people', 'eval', [row['people']])\n",
    "    \n",
    "    for col in people_cols + source_cols:\n",
    "        excel_dict[col] = set()\n",
    "        try:\n",
    "            excel_dict[col] = set(eval(row[col]))\n",
    "        except SyntaxError as e:\n",
    "            assertion_count += 1\n",
    "            assertion_errors += customError('people', 'eval', [row['people']])\n",
    "    \n",
    "    if assertion_count > 0:\n",
    "        raise AssertionError(assertion_errors)\n",
    "        \n",
    "    for i in range(len(people_cols)):\n",
    "        for j in range(i+1, len(people_cols)):\n",
    "            assert excel_dict[people_cols[i]].isdisjoint(excel_dict[people_cols[j]]), customError(\n",
    "                people_cols[i] + ', ' + people_cols[j], 'disjoint')\n",
    "            \n",
    "    for i in range(len(source_cols)):\n",
    "        for j in range(i+1, len(source_cols)):\n",
    "            assert excel_dict[source_cols[i]].isdisjoint(excel_dict[source_cols[j]]), customError(\n",
    "                source_cols[i] + ', ' + source_cols[j], 'disjoint')\n",
    "    \n",
    "    for key in excel_dict.keys():\n",
    "        if not excel_dict[key].issubset(people):\n",
    "            assertion_count += 1\n",
    "            assertion_errors += customError(key, 'subset')\n",
    "        if not people.issuperset(excel_dict[key]):\n",
    "            assertion_count += 1\n",
    "            assertion_errors += customError(key, 'superset')\n",
    "    \n",
    "    if assertion_count > 0:\n",
    "        raise AssertionError(assertion_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "FAILED ASSERTION: Error evaluating people as list, ObjectId(5c5d292b1e67d78e275d9e9f)\n",
      "[\"Ehab Lotayef,\"François Huot\",\"Alexandre Bissonnette\",\"Ibrahima Barry\",\"Mamadou Tanour Barry\",\"Khaled Belkacemi\",\"Aboubake Thabti\",\"Abdelkrim Hassane\",\"Azzedine Soufiane\",\"Dylann Roof\",\"Justin Trudeau\",\"Kenza Tarek\",\"Megda Belkacemi\"]\n",
      "FAILED ASSERTION: Error evaluating people as list, ObjectId(5c5d292b1e67d78e275d9e9f)\n",
      "[\"Ehab Lotayef,\"François Huot\",\"Alexandre Bissonnette\",\"Ibrahima Barry\",\"Mamadou Tanour Barry\",\"Khaled Belkacemi\",\"Aboubake Thabti\",\"Abdelkrim Hassane\",\"Azzedine Soufiane\",\"Dylann Roof\",\"Justin Trudeau\",\"Kenza Tarek\",\"Megda Belkacemi\"]\n",
      "FAILED ASSERTION: Error evaluating people as list, ObjectId(5c5d292b1e67d78e275d9e9f)\n",
      "[\"Ehab Lotayef,\"François Huot\",\"Alexandre Bissonnette\",\"Ibrahima Barry\",\"Mamadou Tanour Barry\",\"Khaled Belkacemi\",\"Aboubake Thabti\",\"Abdelkrim Hassane\",\"Azzedine Soufiane\",\"Dylann Roof\",\"Justin Trudeau\",\"Kenza Tarek\",\"Megda Belkacemi\"]\n",
      "\n",
      "54\n",
      "FAILED ASSERTION: Error evaluating people as list, ObjectId(5c54662d1e67d78e27425afa)\n",
      "[\"Justin Trudeau\",\"Doug Ford\",\"Dalton McGuinty\",\"Adam van Koeverden\",\"Lisa Raitt\"]\n",
      "\n",
      "75\n",
      "FAILED ASSERTION: peopleFemale, peopleMale not disjoint, ObjectId(5c332342795bd2799e61e1ab)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index,row in df.iterrows():\n",
    "    try:\n",
    "        excelAssertions(row)\n",
    "#         print()\n",
    "    except AssertionError as e:\n",
    "        print(index)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OutputJsonFiles/5c1e0b68795bd2a5d03a49a9.json\n",
      "OutputJsonFiles/5c2a3d191e67d78e27b8ac72.json\n",
      "OutputJsonFiles/5c159cb81e67d78e277241fd.json\n",
      "OutputJsonFiles/5c531ba91e67d78e273e272a.json\n",
      "OutputJsonFiles/5c146e42795bd2fcce2ea8e5.json\n",
      "OutputJsonFiles/5c5d292b1e67d78e275d9e9f.json\n",
      "OutputJsonFiles/5c25054e1e67d78e27aac4ef.json\n",
      "OutputJsonFiles/5c2ae5f11e67d78e27ba36d7.json\n",
      "OutputJsonFiles/5c1452701e67d78e276ee126.json\n",
      "OutputJsonFiles/5c286d031e67d78e27b3f17b.json\n",
      "OutputJsonFiles/5c2059ec1e67d78e279ea86c.json\n",
      "OutputJsonFiles/5c1de1661e67d78e27984d34.json\n",
      "OutputJsonFiles/5c201b371e67d78e279e248a.json\n",
      "OutputJsonFiles/5c2060d31e67d78e279eb852.json\n",
      "OutputJsonFiles/5c3e27a31e67d78e27f27b38.json\n",
      "OutputJsonFiles/5c3f78b21e67d78e27f6a477.json\n",
      "OutputJsonFiles/5c546d281e67d78e27426e82.json\n",
      "OutputJsonFiles/5c158f201e67d78e27721ffd.json\n",
      "OutputJsonFiles/5c32f9841e67d78e27cfa4eb.json\n",
      "OutputJsonFiles/5c4a89f31e67d78e27233c5d.json\n",
      "OutputJsonFiles/5c3eec791e67d78e27f51065.json\n",
      "OutputJsonFiles/5c494e541e67d78e271f514e.json\n",
      "OutputJsonFiles/5c3fbf491e67d78e27f767d8.json\n",
      "OutputJsonFiles/5c53fcb21e67d78e2740a299.json\n",
      "OutputJsonFiles/5c4962f31e67d78e271f9498.json\n",
      "OutputJsonFiles/5c48111c1e67d78e271b6146.json\n",
      "OutputJsonFiles/5c2858471e67d78e27b3b633.json\n",
      "OutputJsonFiles/5c5418cc795bd22bf37ca606.json\n",
      "OutputJsonFiles/5c3e11b11e67d78e27f2357a.json\n",
      "OutputJsonFiles/5c5490681e67d78e2742d421.json\n",
      "OutputJsonFiles/5c47fce51e67d78e271b1f7a.json\n",
      "OutputJsonFiles/5c5e50711e67d78e27616b23.json\n",
      "OutputJsonFiles/5c149ffc1e67d78e276fbd44.json\n",
      "OutputJsonFiles/5c3377b81e67d78e27d10a65.json\n",
      "OutputJsonFiles/5c3533311e67d78e27d653f4.json\n",
      "OutputJsonFiles/5c20ae45795bd2d89328853e.json\n",
      "OutputJsonFiles/5c1f1d831e67d78e279c35b4.json\n",
      "OutputJsonFiles/5c5d37341e67d78e275dc30f.json\n",
      "OutputJsonFiles/5c28eba91e67d78e27b54bca.json\n",
      "OutputJsonFiles/5c488fac1e67d78e271d405b.json\n",
      "OutputJsonFiles/5c3d854a1e67d78e27f049d6.json\n",
      "OutputJsonFiles/5c5d3e251e67d78e275e54b5.json\n",
      "OutputJsonFiles/5c5555d11e67d78e27457a93.json\n",
      "OutputJsonFiles/5c29beda1e67d78e27b74939.json\n",
      "OutputJsonFiles/5c498cc6795bd264151080e0.json\n",
      "OutputJsonFiles/5c489df91e67d78e271d66c5.json\n",
      "OutputJsonFiles/5c34d7211e67d78e27d54599.json\n",
      "OutputJsonFiles/5c4977bc1e67d78e27204091.json\n",
      "OutputJsonFiles/5c5314a91e67d78e273e13be.json\n",
      "OutputJsonFiles/5c34c92a1e67d78e27d52117.json\n",
      "OutputJsonFiles/5c1df61f1e67d78e2798f3fe.json\n",
      "OutputJsonFiles/5c33e1a71e67d78e27d2193c.json\n",
      "OutputJsonFiles/5c5da7aa1e67d78e275f8a3c.json\n",
      "OutputJsonFiles/5c3daf32795bd2eb3f0108d8.json\n",
      "OutputJsonFiles/5c53eea41e67d78e27407cc3.json\n",
      "OutputJsonFiles/5c3f55241e67d78e27f63e5a.json\n",
      "OutputJsonFiles/5c2955161e67d78e27b64992.json\n",
      "OutputJsonFiles/5c287b841e67d78e27b4163e.json\n",
      "OutputJsonFiles/5c2a60611e67d78e27b8feef.json\n",
      "OutputJsonFiles/5c1efb3d1e67d78e279bd39a.json\n",
      "OutputJsonFiles/5c3f00a6795bd298e67a078f.json\n",
      "OutputJsonFiles/5c3f4e281e67d78e27f62b50.json\n",
      "OutputJsonFiles/5c339b091e67d78e27d16414.json\n",
      "OutputJsonFiles/5c29ccfc1e67d78e27b76bfb.json\n",
      "OutputJsonFiles/5c483b26795bd2b724e92a68.json\n",
      "OutputJsonFiles/5c3eac6f1e67d78e27f3dc55.json\n",
      "OutputJsonFiles/5c52b36a1e67d78e273d029c.json\n",
      "OutputJsonFiles/5c49ef691e67d78e272197a5.json\n",
      "OutputJsonFiles/5c3e038e1e67d78e27f2105a.json\n",
      "OutputJsonFiles/5c547bf71e67d78e2742971d.json\n",
      "OutputJsonFiles/5c52c73b795bd245ab059d61.json\n",
      "OutputJsonFiles/5c1dccbf1e67d78e279807d8.json\n",
      "OutputJsonFiles/5c2aa6971e67d78e27b9ab24.json\n",
      "OutputJsonFiles/5c29947f1e67d78e27b6d330.json\n",
      "OutputJsonFiles/5c3fefc41e67d78e270257d3.json\n",
      "OutputJsonFiles/5c1dbe1d1e67d78e2797d611.json\n",
      "OutputJsonFiles/5c33859e1e67d78e27d12893.json\n",
      "OutputJsonFiles/5c34c2311e67d78e27d50d44.json\n",
      "OutputJsonFiles/5c332342795bd2799e61e1ab.json\n",
      "OutputJsonFiles/5c49e1261e67d78e2721712b.json\n",
      "OutputJsonFiles/5c1548a31e67d78e2771624f.json\n",
      "OutputJsonFiles/5c3436991e67d78e27d30c2c.json\n",
      "OutputJsonFiles/5c28972a795bd2fac69fa974.json\n",
      "OutputJsonFiles/5c529d681e67d78e273c4cb9.json\n",
      "OutputJsonFiles/5c48278d1e67d78e271c1a28.json\n",
      "OutputJsonFiles/5c54662d1e67d78e27425afa.json\n",
      "OutputJsonFiles/5c344a9f1e67d78e27d34d0a.json\n",
      "OutputJsonFiles/5c3ed6b11e67d78e27f46477.json\n",
      "OutputJsonFiles/5c182ac21e67d78e277944ad.json\n",
      "OutputJsonFiles/5c533fe21e67d78e273e92d1.json\n",
      "OutputJsonFiles/5c1f328f1e67d78e279c7d31.json\n",
      "OutputJsonFiles/5c5d15151e67d78e275d5d0f.json\n",
      "OutputJsonFiles/5c3370aa1e67d78e27d0f869.json\n",
      "OutputJsonFiles/5c1f08711e67d78e279bf66d.json\n",
      "OutputJsonFiles/5c4888ac1e67d78e271d2cdf.json\n",
      "OutputJsonFiles/5c29e8a8795bd2ac48ec6e58.json\n",
      "OutputJsonFiles/5c5d532a795bd2d5c282a094.json\n",
      "OutputJsonFiles/5c3474c2795bd22cf5864830.json\n"
     ]
    }
   ],
   "source": [
    "json_objs = {}\n",
    "\n",
    "for file in json_files:\n",
    "    print(file)\n",
    "    if file.endswith('.json'):\n",
    "        with open(file, 'r+', encoding=ENCODING) as fo:\n",
    "            file_str = fo.read().rstrip()\n",
    "        json_objs[file.split('/')[-1]] = json.loads(file_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_fields = ['verb_index', 'speaker_index', 'quote_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonAssertions(json):\n",
    "    for quote_record in json:\n",
    "        fields = ['verb', 'verb_index', 'quote', 'quote_index', 'speaker',\n",
    "                  'speaker_index', 'reference']\n",
    "        for field in fields:\n",
    "            assert field in quote_record.keys(), \"FAILED ASSERTION: Fields in quote record: \" + field\n",
    "        \n",
    "        assert len(quote_record['quote']) > 0, \"FAILED ASSERTION: Empty quote\"\n",
    "        assert len(quote_record['quote_index']) > 0, \"FAILED ASSERTION: Empty quote-index\"\n",
    "        assert len(quote_record['reference']) > 0, \"FAILED ASSERTION: Empty reference\"\n",
    "        \n",
    "        if len(quote_record['verb']) == 0:\n",
    "            assert len(quote_record['verb_index']) == 0, \"FAILED ASSERTION: verb empty, verb-index non-empty\"\n",
    "        if len(quote_record['verb_index']) == 0:\n",
    "            assert len(quote_record['verb']) == 0, \"FAILED ASSERTION: verb-index non-empty, verb-index empty\"\n",
    "            \n",
    "        if len(quote_record['speaker']) == 0:\n",
    "            assert len(quote_record['speaker_index']) == 0, \"FAILED ASSERTION: speaker empty, speaker-index non-empty\"\n",
    "        if len(quote_record['speaker_index']) == 0:\n",
    "            assert len(quote_record['speaker']) == 0, \"FAILED ASSERTION: speaker non-empty, speaker-index empty\"\n",
    "        \n",
    "        tuples = []\n",
    "        \n",
    "        for field in index_fields:\n",
    "            if len(quote_record[field]) > 0:\n",
    "                index_tuple = eval(quote_record[field])\n",
    "                tuples.append(index_tuple)\n",
    "                \n",
    "                assert index_tuple[1] > index_tuple[0], \"FAILED ASSERTION: bad tuple \" + quote_record[field]\n",
    "                \n",
    "                assert len(quote_record[field.split('_index')[0]]) == index_tuple[1] - index_tuple[0], (\n",
    "                                \"FAILED ASSERTION: \" + field + \" length mismatch \") + quote_record[field]\n",
    "            \n",
    "        tuple_set = set(tuples)\n",
    "        assert len(tuple_set) == len(tuples), \"FAILED ASSERTION: distinct indices\"\n",
    "            \n",
    "        for i,t_1 in enumerate(tuples):\n",
    "            others = tuples[:i] + tuples[i+1:]\n",
    "            for t_2 in others:\n",
    "                assert t_1[0] != t_2[1], \"FAILED ASSERTION: overlap in indices\"\n",
    "                assert t_1[1] != t_2[0], \"FAILED ASSERTION: overlap in indices\"\n",
    "                \n",
    "                \n",
    "        if len(quote_record['verb']) > 0:\n",
    "            assert ' ' not in quote_record['verb'], \"FAILED ASSERTION: verb contains space \" + quote_record['verb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5c5d292b1e67d78e275d9e9f.json\n",
      "FAILED ASSERTION: Empty quote\n",
      "\n",
      "5c25054e1e67d78e27aac4ef.json\n",
      "FAILED ASSERTION: quote_index length mismatch (8652,8663)\n",
      "\n",
      "5c5e50711e67d78e27616b23.json\n",
      "FAILED ASSERTION: quote_index length mismatch (3566,3685)\n",
      "\n",
      "5c5555d11e67d78e27457a93.json\n",
      "FAILED ASSERTION: verb empty, verb-index non-empty\n",
      "\n",
      "5c2a60611e67d78e27b8feef.json\n",
      "FAILED ASSERTION: speaker_index length mismatch (2121,2198)\n",
      "\n",
      "5c1efb3d1e67d78e279bd39a.json\n",
      "FAILED ASSERTION: speaker_index length mismatch (2102,2110)\n",
      "\n",
      "5c29947f1e67d78e27b6d330.json\n",
      "FAILED ASSERTION: bad tuple (7937,3943)\n",
      "\n",
      "5c33859e1e67d78e27d12893.json\n",
      "FAILED ASSERTION: quote_index length mismatch (2136,2192)\n",
      "\n",
      "5c54662d1e67d78e27425afa.json\n",
      "FAILED ASSERTION: quote_index length mismatch (1886,1926)\n",
      "\n",
      "5c4888ac1e67d78e271d2cdf.json\n",
      "FAILED ASSERTION: speaker empty, speaker-index non-empty\n",
      "\n",
      "5c5d532a795bd2d5c282a094.json\n",
      "FAILED ASSERTION: quote_index length mismatch (8769,8858)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fname in json_objs.keys():\n",
    "    try:\n",
    "        jsonAssertions(json_objs[fname])\n",
    "    except AssertionError as e:\n",
    "        print(fname)\n",
    "        print(e)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_list = []\n",
    "for fname in json_objs.keys():\n",
    "    for quote_record in json_objs[fname]:\n",
    "        verb = quote_record['verb']\n",
    "        if len(verb) > 0:\n",
    "            verb_list.append(verb.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chanted', 'reminded', 'explain', 'wrote', 'sign', 'urge', 'reported', 'announced', 'alleged', 'warned', 'tells', 'promise', 'notes', 'noting', 'assured', 'described', 'joking', 'calls', 'maintains', 'acknowledges', 'advised', 'echoes', 'ruled', 'insists', 'recalled', 'joked', 'stuttered', 'noted', 'claims', 'confirmed', 'believed', 'report', 'stated', 'admitted', 'warns', 'insisting', 'alleges', 'remarks', 'lamented', 'continues', 'called', 'explains', 'asks', 'points', 'acknowledged', 'argues', 'replied', 'writing', 'scolded', '“This can happen again.”', 'confessed', 'indicated', 'shout', 'adds', 'suggests', 'said', 'say', 'responded', 'commented', 'told', 'voiced', 'testifying', 'writes', 'added', 'testified', 'argued', 'stressed', 'describing', 'claimed', 'predicted', 'asked', 'advises', 'states', 'calling', 'says', 'went', 'wondered', 'began', 'sniffed', 'urged', 'tweeted', 'repeated', 'complained', 'arguing', 'admits', 'explained', 'talking', 'conceded', 'saying', 'crowed', 'argue', 'describes', 'quipped', 'concludes', 'read', 'retorted', 'blurted', 'adding', 'suggested', 'expressed', 'mentioned']\n"
     ]
    }
   ],
   "source": [
    "print(list(set(verb_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('verbs_fr.txt', 'w+') as fo:\n",
    "    for verb in list(set(verb_list)):\n",
    "        fo.write(verb + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats on quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1493"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_quotes = []\n",
    "for fname in json_objs.keys():\n",
    "    num_quotes.append(len(json_objs[fname]))\n",
    "    \n",
    "sum(num_quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1105"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_syntactic_quotes = []\n",
    "for fname in json_objs.keys():\n",
    "    f_quotes = 0\n",
    "    for quote_record in json_objs[fname]:\n",
    "        if (len(quote_record['verb']) > 0) and (len(quote_record['speaker']) > 0):\n",
    "            f_quotes += 1\n",
    "    num_syntactic_quotes.append(f_quotes)\n",
    "    \n",
    "sum(num_syntactic_quotes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
