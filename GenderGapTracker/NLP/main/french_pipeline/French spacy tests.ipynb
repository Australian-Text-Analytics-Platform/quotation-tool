{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import getopt\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import pymongo\n",
    "import spacy\n",
    "from statistics import mean\n",
    "from bson import ObjectId\n",
    "from spacy.tokens import Span\n",
    "from nltk import Tree\n",
    "import re\n",
    "import math\n",
    "\n",
    "nlp = spacy.load('fr_core_news_md')\n",
    "nlp_coref = spacy.load('en_coref_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILENAME = 'GoldSampleRaw.txt'\n",
    "\n",
    "FILENAME = 'test.txt'\n",
    "\n",
    "# doc_lines = open('../../../../french_subset/' + FILENAME, 'r').readlines()\n",
    "# doc_text = '\\n'.join(doc_lines)\n",
    "\n",
    "OUTPUT_DIRECTORY = '../../../../output/'\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "selon PERSON\n",
    "d'après PERSON\n",
    "\n",
    "argue: faire valoir\n",
    "argued: a fait valoir, ont fait valoir\n",
    "argues: fait valoir, font valoir\n",
    "faire remarquer, (a; ont) fait remarquer; (fait; font) remarquer\n",
    "\n",
    "laisser tomber, (a; ont) laissé tomber, laisse(nt) tomber\n",
    "\n",
    "se questionner, (s'est; se sont) questionné(e; s; es), questionne(nt)\n",
    "s'exprimer, (s'est, se sont) exprimé(e; s; es), s'exprime(nt)\n",
    "se désoler, (s'est; se sont) désolé(e; s; es), se désole(nt)\n",
    "se consoler, (s'est; se sont) consolé(e; s; es), se console(nt)\n",
    "se lamenter, (s'est; se sont) lamenté(e; s; es), se lamente(nt)\n",
    "s'interroger, (s'est; se sont) interrogé(e; s; es); s'interroge(nt)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "quoteVerbWhiteList = ['reconnaitre', 'ajouter', 'admettre', 'annoncer', 'croire', 'prétendre', 'soutenir',\n",
    "                      'conclure', 'confirmer', 'déclarer', 'décrire', 'assurer', 'expliquer', 'trouver',\n",
    "                      'indiquer', 'informer', 'insister', 'noter', 'souligner', 'prédire', 'fournir',\n",
    "                      'divulger', 'rappeler', 'répondre', 'dire', 'rapporter', 'répondre', 'affirmer',\n",
    "                      'suggérer', 'attester', 'penser', 'gazouiller', 'tweeter', 'avertir', 'écrire'] + [\n",
    "                      'révéler', 'commenter', 'avouer', 'raconter', 'prévenir', 'prédire', 'redouter',\n",
    "                      'soulever', 'préciser', 'résumer', 'juger', 'estimer', 'dancer', 'lancer', \n",
    "                      'nuancer', 'relever', 'constater', 'réclamer', 'remarquer', 'confier', 'observer',\n",
    "                      'réagir', 'concéder', 'témoigner', 'louanger', 'demander', 'arguer', 'protester',\n",
    "                      'critiquer', 'plaider', 'poursuivre', 'trancher', 'mentionner', 'souhaiter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_GUILLEMETS = set()\n",
    "END_GUILLEMETS = set()\n",
    "\n",
    "# ----- Formatting Functions\n",
    "def get_pretty_index(key):\n",
    "    frmt = '({0},{1})'\n",
    "    if isinstance(key, spacy.tokens.span.Span):\n",
    "        return frmt.format(key.start_char, key.end_char)\n",
    "    elif isinstance(key, spacy.tokens.token.Token):\n",
    "        return frmt.format(key.idx, key.idx + len(key.text))\n",
    "    \n",
    "def prettify(key):\n",
    "    frmt = '{0} ({1},{2})'\n",
    "\n",
    "    if isinstance(key, spacy.tokens.span.Span):\n",
    "        return frmt.format(str(key), key.start_char, key.end_char)\n",
    "    elif isinstance(key, spacy.tokens.token.Token):\n",
    "        return frmt.format(str(key), key.idx, key.idx + len(key.text))\n",
    "    \n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return node.orth_ + '[' + node.dep_ + ']'\n",
    "    \n",
    "def preprocess_text(txt):\n",
    "    txt = txt.replace(u'\\xa0', u' ')\n",
    "#     txt = txt.replace(u'\\xe9', u' ')\n",
    "    # To fix the problem of not breaking at \\n\n",
    "    txt = txt.replace(\"\\n\", \".\\n \")\n",
    "    # To remove potential duplicate dots\n",
    "    txt = txt.replace(\"..\\n \", \".\\n \")\n",
    "    txt = txt.replace(\". .\\n \", \".\\n \")\n",
    "    txt = txt.replace(\"  \", \" \")\n",
    "    # Replace double quotes\n",
    "    txt = txt.replace(\"”\",'\"')\n",
    "    txt = txt.replace(\"“\",'\"')\n",
    "    # ---\n",
    "    txt = txt.replace(\"〝\",'\"')\n",
    "    txt = txt.replace(\"〞\",'\"')\n",
    "    # Replace single quotes\n",
    "    #txt = txt.replace(\"‘\",\"'\")\n",
    "    #txt = txt.replace(\"’\",''')\n",
    "    \n",
    "    # Replace guillemets with spaces\n",
    "    txt = txt.replace(\"« \",'«')\n",
    "    txt = txt.replace(\" »\",'»')\n",
    "    \n",
    "    # Note positions of all start and end guillemets\n",
    "    for match in re.finditer('«', txt):\n",
    "        START_GUILLEMETS.add(match.start())\n",
    "    for match in re.finditer('»', txt):\n",
    "        END_GUILLEMETS.add(match.end())\n",
    "    \n",
    "    # Replace guillemets\n",
    "    txt = txt.replace(\"«\",'\"')\n",
    "    txt = txt.replace(\"»\",'\"')\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_quotes(doc_id, doc, write_tree=False):\n",
    "    syntactic_quotes = extract_syntactic_quotes(doc_id, doc, write_tree)\n",
    "    reversed_quotes = extract_reversed_quotes(doc_id, doc, syntactic_quotes, write_tree)\n",
    "    selon_quotes = extract_selon_quotes(doc_id, doc, syntactic_quotes + reversed_quotes, write_tree)\n",
    "    floating_quotes = extract_floating_quotes(doc, syntactic_quotes + reversed_quotes + selon_quotes)\n",
    "    one_sided_quotes = extract_one_sided_quotes(doc, syntactic_quotes + reversed_quotes + selon_quotes + floating_quotes)\n",
    "\n",
    "    return syntactic_quotes + reversed_quotes + selon_quotes + floating_quotes + one_sided_quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quote_type(doc, quote, verb, speaker, subtree_span):\n",
    "    dc1_pos = -1\n",
    "    dc2_pos = -1\n",
    "    quote_starts_with_quote = False\n",
    "    quote_ends_with_quote = False\n",
    "\n",
    "    if (doc[max(0, quote.start - 1)].is_quote or doc[quote.start].is_quote) and (\n",
    "            doc[min(len(doc) - 1, quote.end)].is_quote or doc[min(len(doc) - 1, quote.end + 1)].is_quote):\n",
    "        quote_starts_with_quote = True\n",
    "        quote_ends_with_quote = True\n",
    "        dc1_pos = max(0, quote.start_char - 1)\n",
    "        dc2_pos = min(len(doc) - 1, quote.end + 1)\n",
    "    elif doc[max(0, subtree_span.start - 1)].is_quote and doc[min(len(doc) - 1, subtree_span.end + 1)].is_quote:\n",
    "        quote_starts_with_quote = True\n",
    "        quote_ends_with_quote = True\n",
    "        dc1_pos = max(0, subtree_span.start_char - 1)\n",
    "        dc2_pos = min(len(doc) - 1, quote.end + 1)\n",
    "    elif speaker.start < quote.start and doc[max(0, speaker.start - 1)].is_quote and doc[\n",
    "        min(len(doc) - 1, subtree_span.end + 1)].is_quote:\n",
    "        quote_starts_with_quote = True\n",
    "        quote_ends_with_quote = True\n",
    "        dc1_pos = max(0, speaker.start_char - 1)\n",
    "        dc2_pos = min(len(doc) - 1, quote.end + 1)\n",
    "\n",
    "    content_pos = mean([quote.start_char, quote.end_char])\n",
    "    verb_pos = mean([verb.idx, verb.idx + len(str(verb))])\n",
    "    speaker_pos = mean([speaker.start_char, speaker.end_char])\n",
    "\n",
    "    # phrase = subtree_span\n",
    "    # print('---')\n",
    "    # print(phrase)\n",
    "    # print(quote_starts_with_quote, quote_ends_with_quote, \" | \", phrase[0].is_quote, phrase[-1].is_quote)\n",
    "    # print(\"-> | \", dc1_pos, dc2_pos, content_pos, verb_pos, speaker_pos)\n",
    "    # print()\n",
    "\n",
    "    if quote_starts_with_quote and quote_ends_with_quote:\n",
    "        letters = [\"Q\", \"q\", \"C\", \"V\", \"S\"]\n",
    "        indices = [dc1_pos, dc2_pos, content_pos, verb_pos, speaker_pos]\n",
    "    else:\n",
    "        letters = [\"C\", \"V\", \"S\"]\n",
    "        indices = [content_pos, verb_pos, speaker_pos]\n",
    "\n",
    "    keydict = dict(zip(letters, indices))\n",
    "    letters.sort(key=keydict.get)\n",
    "    return \"\".join(letters).replace('q', 'Q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seenBefore(regex_match, quote_objects):\n",
    "    quotations = set([eval(quote_obj['quote_index']) for quote_obj in quote_objects])\n",
    "    \n",
    "    for q in quotations:\n",
    "        if fuzzy_match(regex_match.start(), int(q[0])) and fuzzy_match(regex_match.end(), int(q[1])):\n",
    "            # It's a match, so disregard it\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def fuzzy_match(num1, num2):\n",
    "    return abs(num1 - num2) < 3\n",
    "\n",
    "def getSentenceNumber(sentence_dict, char):\n",
    "    keys_subset = [k for k in sentence_dict.keys() if k <= char]\n",
    "    assert len(keys_subset) > 0\n",
    "    sentence_char = max(keys_subset)\n",
    "    return sentence_dict[sentence_char]\n",
    "\n",
    "def getClosestPreceding(preceding_dict, char):\n",
    "    keys_subset = [k for k in preceding_dict.keys() if k <= char]\n",
    "    if len(keys_subset) > 0:\n",
    "        return max(keys_subset)\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def getClosestFollowing(following_dict, char):\n",
    "    keys_subset = [k for k in following_dict.keys() if k >= char]\n",
    "    if len(keys_subset) > 0:\n",
    "        return min(keys_subset)\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def extract_reversed_quotes(doc_id, doc, syntactic_quotes, write_tree=False):\n",
    "    quote_list = []\n",
    "    \n",
    "    if write_tree:\n",
    "        tree_writer = open(os.path.join(OUTPUT_DIRECTORY, doc_id + '.txt'), 'w')\n",
    "        \n",
    "    # Named entity preprocessing\n",
    "    # Create a named entity dictionary by sentence\n",
    "    named_people_dict = {}\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PER':\n",
    "            named_people_dict[ent.start_char] = ent\n",
    "    \n",
    "    # spaCy wrappers\n",
    "    # Create a dictionary of sentences, sentence numbers, indexed by start character\n",
    "    sentence_dict = {}\n",
    "    for i,sent in enumerate(doc.sents):\n",
    "        sentence_dict[sent.start_char] = i,sent\n",
    "        \n",
    "    # Noun chunk preprocessing\n",
    "    # Create a dictionary of noun chunks indexed by start character\n",
    "    noun_chunk_dict = {}\n",
    "    for noun_chunk in doc.noun_chunks:\n",
    "        noun_chunk_dict[noun_chunk.start_char] = noun_chunk\n",
    "    \n",
    "    for token in doc:\n",
    "        if (token.pos_ == 'PRON') and (token.idx not in noun_chunk_dict.keys()):\n",
    "            span = doc[token.i : token.i+1]\n",
    "            #print(span)\n",
    "            noun_chunk_dict[span.start_char] = span\n",
    "    \n",
    "    # Find list of quotes with quotation marks\n",
    "    regex_quotes = []\n",
    "    # Prune according to existing list of syntactic quotes (fuzzy match)\n",
    "    for match in re.finditer('(?<=\")[^\"]+(?=\")', doc.text):\n",
    "        # ignore quotes that don't start or end with the right kind of quote char\n",
    "        if match.start()-1 not in START_GUILLEMETS:\n",
    "            continue\n",
    "        if match.end()+1 not in END_GUILLEMETS:\n",
    "            continue\n",
    "        \n",
    "        #print(match.start(), match.end())\n",
    "        if not seenBefore(match, syntactic_quotes):\n",
    "            regex_quotes.append(match)\n",
    "    \n",
    "    for q in regex_quotes:\n",
    "        # Find the sentence of the extracted quotes (with high probability, all tokens will be same sentence)\n",
    "        sentence_number, _ = getSentenceNumber(sentence_dict,\n",
    "                                               q.start())\n",
    "        sentence_number_end, _ = getSentenceNumber(sentence_dict,\n",
    "                                                   q.end())\n",
    "        \n",
    "        span = doc.char_span(q.start(), q.end())\n",
    "        quote_token_count = len(span)\n",
    "        \n",
    "        # Find the closest named person following the sentence to link if possible\n",
    "        closest_person_start_char = getClosestFollowing(named_people_dict,\n",
    "                                                          q.end())\n",
    "\n",
    "        closest_person_sentence_number_end = -1\n",
    "        \n",
    "        if closest_person_start_char != -1:\n",
    "            closest_person_sentence_number, _ = getSentenceNumber(sentence_dict,\n",
    "                                                              closest_person_start_char)\n",
    "        \n",
    "            closest_person_sentence_number_end, _ = getSentenceNumber(sentence_dict,\n",
    "                                                                      named_people_dict[closest_person_start_char].end_char)\n",
    "        \n",
    "            #print('DEBUG:', sentence_number, sentence_number_end, closest_person_sentence_number, closest_person_sentence_number_end)\n",
    "        \n",
    "            # Best case scenario: named person and the end of quote are in the same sentence\n",
    "            if closest_person_sentence_number == sentence_number_end:\n",
    "                person = named_people_dict[closest_person_start_char]\n",
    "                #print(q.start(), q.end(), sentence_number, person)\n",
    "                \n",
    "                quote_obj = {\n",
    "                    'speaker': person.text,\n",
    "                    'speaker_index': '({0},{1})'.format(person.start_char, person.end_char),\n",
    "                    'quote': q.group(0),\n",
    "                    'quote_index': '({0},{1})'.format(q.start(), q.end()),\n",
    "                    'verb': '',\n",
    "                    'verb_index': '',\n",
    "                    'quote_token_count': quote_token_count,\n",
    "                    'quote_type': 'QCQVS',\n",
    "                    'is_floating_quote': False,\n",
    "                    'reference': noun_chunk.text\n",
    "                }\n",
    "                \n",
    "                quote_list.append(quote_obj)\n",
    "                continue\n",
    "        \n",
    "        # Otherwise try to find a noun phrase in the sentence - Doc.noun_chunks (e.g., \"la mairesse de Bobigny\")\n",
    "        closest_noun_chunk_start_char = getClosestFollowing(noun_chunk_dict,\n",
    "                                                            q.end())\n",
    "        \n",
    "        if closest_noun_chunk_start_char != -1:\n",
    "            closest_noun_chunk_sentence_number, _ = getSentenceNumber(sentence_dict,\n",
    "                                                                      closest_noun_chunk_start_char)\n",
    "        \n",
    "            closest_noun_chunk_sentence_number_end, _ = getSentenceNumber(sentence_dict,\n",
    "                                                                          noun_chunk_dict[closest_noun_chunk_start_char].end_char)\n",
    "        \n",
    "            #print('DEBUG:', sentence_number, sentence_number_end, closest_noun_chunk_sentence_number, closest_noun_chunk_sentence_number_end)\n",
    "        \n",
    "            # Best case scenario: noun chunk and the end of quote are in the same sentence\n",
    "            if closest_noun_chunk_sentence_number == sentence_number_end:\n",
    "                noun_chunk = noun_chunk_dict[closest_noun_chunk_start_char]\n",
    "                #print(q.start(), q.end(), sentence_number, noun_chunk)\n",
    "                \n",
    "                quote_obj = {\n",
    "                    'speaker': noun_chunk.text,\n",
    "                    'speaker_index': '({0},{1})'.format(noun_chunk.start_char, noun_chunk.end_char),\n",
    "                    'quote': q.group(0),\n",
    "                    'quote_index': '({0},{1})'.format(q.start(), q.end()),\n",
    "                    'verb': '',\n",
    "                    'verb_index': '',\n",
    "                    'quote_token_count': quote_token_count,\n",
    "                    'quote_type': 'QCQVS',\n",
    "                    'is_floating_quote': False,\n",
    "                    'reference': noun_chunk.text\n",
    "                }\n",
    "                \n",
    "                quote_list.append(quote_obj)\n",
    "                continue\n",
    "        \n",
    "        # Otherwise leave it for the floating quote stage\n",
    "    \n",
    "    return quote_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = 'test.txt'\n",
    "\n",
    "doc_lines = open('../../../../french_subset/' + FILENAME, 'r').readlines()\n",
    "doc_text = '\\n'.join(doc_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasAlpha(text):\n",
    "    if text is None:\n",
    "        return False\n",
    "    for letter in text:\n",
    "        if letter.isalpha():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def extract_syntactic_quotes(doc_id, doc, write_tree=False):\n",
    "    quote_list = []\n",
    "    if write_tree:\n",
    "        tree_writer = open(os.path.join(OUTPUT_DIRECTORY, doc_id + '.txt'), 'w')\n",
    "    for word in doc:\n",
    "        if word.dep_ in ('ccomp'):\n",
    "            subtree_span = doc[word.left_edge.i: word.right_edge.i + 1]\n",
    "#             print(word.dep_, '|', subtree_span.text, '|', subtree_span.root.head.text)\n",
    "            sent = subtree_span\n",
    "            sent_str = str(sent)\n",
    "            verb = subtree_span.root.head\n",
    "            speaker = \"\"\n",
    "            for child in subtree_span.root.head.children:\n",
    "                #print(verb.lemma_.lower())\n",
    "                if (child.dep_ == 'nsubj') and (verb.lemma_.lower() in quoteVerbWhiteList):\n",
    "                    subj_subtree_span = doc[child.left_edge.i: child.right_edge.i + 1]\n",
    "                    speaker = subj_subtree_span\n",
    "                    if type(speaker) == spacy.tokens.span.Span:\n",
    "                        quote_length = len(sent)\n",
    "                        speaker_index = get_pretty_index(speaker)\n",
    "                        quote_type = get_quote_type(doc, sent, verb, speaker, subtree_span)\n",
    "                        # Filter invalid quotes (mostly floating quotes detected with invalid speaker/verb)\n",
    "\n",
    "                        is_valid_speaker = str(speaker).strip().lower() not in [\"je\",\"nous\"]\n",
    "                        is_valid_type = not (quote_type[0] == \"Q\" and quote_type[-1] == \"Q\")\n",
    "                        is_valid_quote = len(sent_str.strip()) > 0\n",
    "                        if (is_valid_quote and is_valid_type and is_valid_speaker):\n",
    "                        #if is_valid_type:\n",
    "                            quote_obj = {\n",
    "                                'speaker': str(speaker),\n",
    "                                'speaker_index': speaker_index,\n",
    "                                'quote': sent_str,\n",
    "                                'quote_index': get_pretty_index(sent),\n",
    "                                'verb': str(verb),\n",
    "                                'verb_index': get_pretty_index(verb),\n",
    "                                'quote_token_count': quote_length,\n",
    "                                'quote_type': quote_type,\n",
    "                                'is_floating_quote': False,\n",
    "                                'reference': str(speaker)\n",
    "                            }\n",
    "                            quote_list.append(quote_obj)\n",
    "\n",
    "                            if write_tree:\n",
    "                                tree_writer.writelines(\n",
    "                                    '{0}\\n{1}\\n{0}\\n'.format('-' * (len(sent_str) + 1), sent_str.replace('\\n', ' ')))\n",
    "                                quote_tree_string = to_nltk_tree(subtree_span.root.head).pretty_print(\n",
    "                                    stream=tree_writer)\n",
    "                        break\n",
    "                \n",
    "    return quote_list\n",
    "\n",
    "def extract_floating_quotes(doc, quotations):\n",
    "    \n",
    "    floating_quotes = []\n",
    "    \n",
    "    regex_quotes = []\n",
    "    for match in re.finditer('(?<=\")[^\"]+(?=\")', doc.text):\n",
    "        # ignore quotes that don't start or end with the right kind of quote char\n",
    "        if match.start()-1 not in START_GUILLEMETS:\n",
    "            continue\n",
    "        if match.end()+1 not in END_GUILLEMETS:\n",
    "            continue\n",
    "            \n",
    "        if not seenBefore(match, quotations):\n",
    "            regex_quotes.append(match)\n",
    "    \n",
    "    # spaCy wrappers\n",
    "    # Create a dictionary of sentences, sentence numbers, indexed by start character\n",
    "    sentence_dict = {}\n",
    "    for i,sent in enumerate(doc.sents):\n",
    "        sentence_dict[sent.start_char] = i,sent\n",
    "        \n",
    "    # Create a dictionary of quotes, indexed by final character\n",
    "    quotation_dict = {}\n",
    "    for quotation in quotations:\n",
    "        indices = []\n",
    "        for index in ['quote_index', 'verb_index', 'speaker_index']:\n",
    "            if len(quotation[index]) > 0:\n",
    "                indices.append(eval(quotation[index])[1])\n",
    "\n",
    "        quotation_dict[max(indices)] = quotation['speaker']\n",
    "    \n",
    "    for q in regex_quotes:\n",
    "        \n",
    "        previous_quotation_index = getClosestPreceding(quotation_dict, q.start())\n",
    "        assert previous_quotation_index < q.start()\n",
    "        \n",
    "        if previous_quotation_index != -1:\n",
    "            candidate_speaker = quotation_dict[previous_quotation_index]\n",
    "            #print(candidate_speaker)\n",
    "            \n",
    "            span = doc.text[previous_quotation_index:q.start()]\n",
    "            assert span is not None\n",
    "            #print(previous_quotation_index, q.start())\n",
    "            \n",
    "            if not hasAlpha(span):\n",
    "                span = doc.char_span(q.start(), q.end())\n",
    "                quote_token_count = len(span)\n",
    "\n",
    "                quote_obj = {\n",
    "                    'speaker': '',\n",
    "                    'speaker_index': '',\n",
    "                    'quote': q.group(0),\n",
    "                    'quote_index': '({0},{1})'.format(q.start(), q.end()),\n",
    "                    'verb': '',\n",
    "                    'verb_index': '',\n",
    "                    'quote_token_count': quote_token_count,\n",
    "                    'quote_type': 'QCQ',\n",
    "                    'is_floating_quote': True,\n",
    "                    'reference': candidate_speaker\n",
    "                }\n",
    "\n",
    "                floating_quotes.append(quote_obj)\n",
    "\n",
    "    return floating_quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_one_sided_quotes(doc, quotations):\n",
    "    \n",
    "    one_sided_quotes = []\n",
    "    \n",
    "    regex_quotes = []\n",
    "    for match in re.finditer('(?<=\")[^\"\\n]+(?=\\n)', doc.text):\n",
    "#         print(match.group(0))\n",
    "        \n",
    "        # ignore quotes that don't start with the right kind of quote char\n",
    "        if match.start()-1 not in START_GUILLEMETS:\n",
    "            continue\n",
    "        \n",
    "        if not seenBefore(match, quotations):\n",
    "            regex_quotes.append(match)\n",
    "    \n",
    "    # spaCy wrappers\n",
    "    # Create a dictionary of sentences, sentence numbers, indexed by start character\n",
    "    sentence_dict = {}\n",
    "    for i,sent in enumerate(doc.sents):\n",
    "        sentence_dict[sent.start_char] = i,sent\n",
    "        \n",
    "    # Create a dictionary of quotes, indexed by final character\n",
    "    quotation_dict = {}\n",
    "    for quotation in quotations:\n",
    "        indices = []\n",
    "        for index in ['quote_index', 'verb_index', 'speaker_index']:\n",
    "            if len(quotation[index]) > 0:\n",
    "                indices.append(eval(quotation[index])[1])\n",
    "\n",
    "        quotation_dict[max(indices)] = quotation['speaker']\n",
    "    \n",
    "    for q in regex_quotes:\n",
    "        \n",
    "        previous_quotation_index = getClosestPreceding(quotation_dict, q.start())\n",
    "        assert previous_quotation_index < q.start()\n",
    "        \n",
    "        if previous_quotation_index != -1:\n",
    "            candidate_speaker = quotation_dict[previous_quotation_index]\n",
    "            #print(candidate_speaker)\n",
    "            \n",
    "            span = doc.text[previous_quotation_index:q.start()]\n",
    "            assert span is not None\n",
    "            #print(previous_quotation_index, q.start())\n",
    "            \n",
    "            if not hasAlpha(span):\n",
    "                span = doc.char_span(q.start(), q.end())\n",
    "                quote_token_count = len(span)\n",
    "\n",
    "                quote_obj = {\n",
    "                    'speaker': '',\n",
    "                    'speaker_index': '',\n",
    "                    'quote': q.group(0),\n",
    "                    'quote_index': '({0},{1})'.format(q.start(), q.end()),\n",
    "                    'verb': '',\n",
    "                    'verb_index': '',\n",
    "                    'quote_token_count': quote_token_count,\n",
    "                    'quote_type': 'QC',\n",
    "                    'is_floating_quote': True,\n",
    "                    'reference': candidate_speaker\n",
    "                }\n",
    "\n",
    "                one_sided_quotes.append(quote_obj)\n",
    "\n",
    "    return one_sided_quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'speaker': 'Jean Winkler4', 'speaker_index': '(193,206)', 'quote': ', \"Les Canadiens sont bons', 'quote_index': '(210,236)', 'verb': 'dit', 'verb_index': '(207,210)', 'quote_token_count': 6, 'quote_type': 'SVC', 'is_floating_quote': False}\n",
      "--------------------------------------------------\n",
      "{'speaker': 'Jean Winkler5', 'speaker_index': '(243,256)', 'quote': '\"Les Canadiens sont bons', 'quote_index': '(261,285)', 'verb': 'dit', 'verb_index': '(257,260)', 'quote_token_count': 5, 'quote_type': 'QSVQC', 'is_floating_quote': False}\n",
      "--------------------------------------------------\n",
      "{'speaker': 'Jean Winkler6', 'speaker_index': '(292,305)', 'quote': 'que les Canadiens sont bons', 'quote_index': '(311,338)', 'verb': 'dit', 'verb_index': '(306,309)', 'quote_token_count': 5, 'quote_type': 'SVC', 'is_floating_quote': False}\n",
      "--------------------------------------------------\n",
      "{'speaker': 'Jean Winkler7', 'speaker_index': '(344,357)', 'quote': 'que les Canadiens sont bons', 'quote_index': '(362,389)', 'verb': 'dit', 'verb_index': '(358,361)', 'quote_token_count': 5, 'quote_type': 'SVC', 'is_floating_quote': False}\n",
      "--------------------------------------------------\n",
      "{'speaker': 'Hao Chen', 'speaker_index': '(1043,1051)', 'quote': 'que les chinois mangent du poisson aussi', 'quote_index': '(1056,1096)', 'verb': 'dit', 'verb_index': '(1052,1055)', 'quote_token_count': 7, 'quote_type': 'SVC', 'is_floating_quote': False}\n",
      "--------------------------------------------------\n",
      "{'speaker': 'Jean Winkler1', 'speaker_index': '(31,44)', 'quote': 'Les Canadiens sont bons', 'quote_index': '(1,24)', 'verb': '', 'verb_index': '', 'quote_token_count': 4, 'quote_type': 'QCQVS', 'is_floating_quote': False}\n",
      "--------------------------------------------------\n",
      "{'speaker': 'la mairesse3 de Bobigny', 'speaker_index': '(129,152)', 'quote': 'Les Canadiens sont bons', 'quote_index': '(99,122)', 'verb': '', 'verb_index': '', 'quote_token_count': 4, 'quote_type': 'QCQVS', 'is_floating_quote': False}\n",
      "--------------------------------------------------\n",
      "{'speaker': '-il', 'speaker_index': '(435,438)', 'quote': 'Les chinois mangent du riz.', 'quote_index': '(402,429)', 'verb': '', 'verb_index': '', 'quote_token_count': 6, 'quote_type': 'QCQVS', 'is_floating_quote': False}\n",
      "--------------------------------------------------\n",
      "{'speaker': '-il', 'speaker_index': '(484,487)', 'quote': 'Les Canadiens mangent du pain', 'quote_index': '(449,478)', 'verb': '', 'verb_index': '', 'quote_token_count': 5, 'quote_type': 'QCQVS', 'is_floating_quote': False}\n",
      "--------------------------------------------------\n",
      "{'speaker': 'Jean Winkler8', 'speaker_index': '(589,602)', 'quote': 'Les Canadiens mangent du pain', 'quote_index': '(551,580)', 'verb': '', 'verb_index': '', 'quote_token_count': 5, 'quote_type': 'QCQVS', 'is_floating_quote': False}\n",
      "--------------------------------------------------\n",
      "{'speaker': 'Jean Winkler8', 'speaker_index': '(589,602)', 'quote': '\"Les Canadiens mangent du pain\",', 'quote_index': '(550,582)', 'verb': '', 'verb_index': '', 'quote_token_count': 8, 'quote_type': 'QCQVS', 'is_floating_quote': False}\n",
      "--------------------------------------------------\n",
      "{'speaker': 'Jean Winkler9', 'speaker_index': '(645,658)', 'quote': 'Les Canadiens mangent du pain,', 'quote_index': '(608,638)', 'verb': '', 'verb_index': '', 'quote_token_count': 6, 'quote_type': 'QCQVS', 'is_floating_quote': False}\n",
      "--------------------------------------------------\n",
      "{'speaker': 'Jean Winkler10', 'speaker_index': '(703,717)', 'quote': 'Les Canadiens mangent du pain,', 'quote_index': '(664,694)', 'verb': '', 'verb_index': '', 'quote_token_count': 6, 'quote_type': 'QCQVS', 'is_floating_quote': False}\n",
      "--------------------------------------------------\n",
      "{'speaker': 'Jean Winkler11', 'speaker_index': '(729,743)', 'quote': ', les Canadiens mangent du pain', 'quote_index': '(743,774)', 'verb': '', 'verb_index': '', 'quote_token_count': 6, 'quote_type': 'QCQVS', 'is_floating_quote': False}\n",
      "--------------------------------------------------\n",
      "{'speaker': 'Jean Winkler12', 'speaker_index': '(786,800)', 'quote': ', \"Les Canadiens mangent du pain\"', 'quote_index': '(800,833)', 'verb': '', 'verb_index': '', 'quote_token_count': 8, 'quote_type': 'QCQVS', 'is_floating_quote': False}\n",
      "--------------------------------------------------\n",
      "{'speaker': 'Jean Winkler13', 'speaker_index': '(847,861)', 'quote': ', \"Les Canadiens mangent du pain\"', 'quote_index': '(861,894)', 'verb': '', 'verb_index': '', 'quote_token_count': 8, 'quote_type': 'QCQVS', 'is_floating_quote': False}\n",
      "--------------------------------------------------\n",
      "{'speaker': '', 'speaker_index': '', 'quote': 'Les Indiens aussi sont bons', 'quote_index': '(159,186)', 'verb': '', 'verb_index': '', 'quote_token_count': 5, 'quote_type': 'QCQ', 'is_floating_quote': True}\n",
      "--------------------------------------------------\n",
      "{'speaker': '', 'speaker_index': '', 'quote': 'Les chinois boivent du thé.', 'quote_index': '(1103,1130)', 'verb': '', 'verb_index': '', 'quote_token_count': 6, 'quote_type': 'QCQ', 'is_floating_quote': True}\n",
      "--------------------------------------------------\n",
      "{'speaker': '', 'speaker_index': '', 'quote': 'Les chinois mangent du riz.', 'quote_index': '(907,934)', 'verb': '', 'verb_index': '', 'quote_token_count': 6, 'quote_type': 'QCQ', 'is_floating_quote': True}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    doc_text = preprocess_text(doc_text)\n",
    "    doc = nlp(doc_text)\n",
    "    assert (doc[-1].idx + len(doc[-1])) == len(doc.text)\n",
    "    quotes = extract_quotes(doc_id=FILENAME, doc=doc, write_tree=True)\n",
    "    \n",
    "    for q in quotes:\n",
    "        q.pop('reference')\n",
    "        print(q)\n",
    "        print('-' * 50)\n",
    "except:\n",
    "    print('EXCEPTION')\n",
    "    logging.exception(\"message\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stripWithIndices(string, start_index, end_index):\n",
    "    _string = ''\n",
    "    for i,letter in enumerate(string):\n",
    "        if letter.isspace():\n",
    "            continue\n",
    "        _string = string[i:]\n",
    "        start_index = start_index + i\n",
    "        break\n",
    "        \n",
    "    for i,letter in enumerate(reversed(_string)):\n",
    "        if letter.isspace():\n",
    "            continue\n",
    "        string = _string[:len(_string) - i]\n",
    "        end_index = end_index - i\n",
    "        break\n",
    "        \n",
    "    return string, start_index, end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('blah honey noob mug', 205, 224)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stripWithIndices('     blah honey noob mug ', 200, 200+len('     blah honey noob mug '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('blah honey noob mug', 200, 219)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stripWithIndices('blah honey noob mug', 200, 200+len('blah honey noob mug'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_selon_quotes(doc_id, doc, previous_quotes, write_tree=False):\n",
    "    quote_list = []\n",
    "    \n",
    "    if write_tree:\n",
    "        tree_writer = open(os.path.join(OUTPUT_DIRECTORY, doc_id + '.txt'), 'w')\n",
    "        \n",
    "    # Named entity preprocessing\n",
    "    # Create a named entity dictionary by sentence\n",
    "    named_people_dict = {}\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PER':\n",
    "            named_people_dict[ent.start_char] = ent\n",
    "        \n",
    "    # Noun chunk preprocessing\n",
    "    # Create a dictionary of noun chunks indexed by start character\n",
    "    noun_chunk_dict = {}\n",
    "    for noun_chunk in doc.noun_chunks:\n",
    "        noun_chunk_dict[noun_chunk.start_char] = noun_chunk\n",
    "    \n",
    "    for token in doc:\n",
    "        if (token.pos_ == 'PRON') and (token.idx not in noun_chunk_dict.keys()):\n",
    "            span = doc[token.i : token.i+1]\n",
    "            #print(span)\n",
    "            noun_chunk_dict[span.start_char] = span\n",
    "    \n",
    "    # Find list of quotes with quotation marks\n",
    "    selon_quotes = []\n",
    "    # Prune according to existing list of syntactic quotes (fuzzy match)\n",
    "    for match in re.finditer(\"\\s*([^\\.\\n]*([\\s^](?:[sS]elon|[Dd]'après)\\s)[^\\.\\n]*)\\s*\", doc.text):\n",
    "        if not seenBefore(match, previous_quotes):\n",
    "            selon_quotes.append(match)\n",
    "    \n",
    "    for q in selon_quotes:\n",
    "#         print(q)\n",
    "        # Find the closest named person following the sentence to link if possible\n",
    "        closest_person_start_char = getClosestFollowing(named_people_dict,\n",
    "                                                          q.start())\n",
    "        # Find a noun phrase in the sentence - Doc.noun_chunks (e.g., \"la mairesse de Bobigny\")\n",
    "        closest_noun_chunk_start_char = getClosestFollowing(noun_chunk_dict,\n",
    "                                                            q.start())\n",
    "        \n",
    "        if closest_person_start_char != -1:\n",
    "            person = named_people_dict[closest_person_start_char]\n",
    "        elif closest_person_start_char != -1:\n",
    "            person = noun_chunk_dict[closest_noun_chunk_start_char]\n",
    "        else:\n",
    "            print(q)\n",
    "            continue\n",
    "            \n",
    "        # Figuring out the quote_content\n",
    "        beforeSelon = doc.text[q.start(1):q.start(2)]\n",
    "        stripped, start_index, end_index = stripWithIndices(beforeSelon, q.start(1), q.start(2))\n",
    "        if hasAlpha(stripped):\n",
    "            quote_content = stripped\n",
    "            quote_content_start = start_index\n",
    "            quote_content_end = end_index\n",
    "                        \n",
    "            span = doc.char_span(quote_content_start, quote_content_end)\n",
    "            assert span is not None\n",
    "            quote_token_count = len(span)\n",
    "            \n",
    "            quote_obj = {\n",
    "                'speaker': person.text,\n",
    "                'speaker_index': '({0},{1})'.format(person.start_char, person.end_char),\n",
    "                'quote': quote_content,\n",
    "                'quote_index': '({0},{1})'.format(quote_content_start, quote_content_end),\n",
    "                'verb': '',\n",
    "                'verb_index': '',\n",
    "                'quote_token_count': quote_token_count,\n",
    "                'quote_type': 'selon',\n",
    "                'is_floating_quote': False,\n",
    "                'reference': person.text\n",
    "            }\n",
    "            quote_list.append(quote_obj)\n",
    "            \n",
    "        afterSelon = doc.text[person.end_char:q.end(1)]\n",
    "        stripped, start_index, end_index = stripWithIndices(afterSelon, person.end_char, q.end(1))\n",
    "        if hasAlpha(stripped):\n",
    "            quote_content = afterSelon\n",
    "            quote_content_start = start_index\n",
    "            quote_content_end = end_index\n",
    "            \n",
    "            span = doc.char_span(quote_content_start, quote_content_end)\n",
    "            assert span is not None\n",
    "            quote_token_count = len(span)\n",
    "            \n",
    "            quote_obj = {\n",
    "                'speaker': person.text,\n",
    "                'speaker_index': '({0},{1})'.format(person.start_char, person.end_char),\n",
    "                'quote': quote_content,\n",
    "                'quote_index': '({0},{1})'.format(quote_content_start, quote_content_end),\n",
    "                'verb': '',\n",
    "                'verb_index': '',\n",
    "                'quote_token_count': quote_token_count,\n",
    "                'quote_type': 'selon',\n",
    "                'is_floating_quote': False,\n",
    "                'reference': person.text\n",
    "            }\n",
    "            quote_list.append(quote_obj)\n",
    "        \n",
    "        # Otherwise leave it for the floating quote stage\n",
    "    \n",
    "    return quote_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Les Canadiens sont bons\", dit Jean Winkler1.\n",
       " .\n",
       " Les Canadiens sont bons, dit Jean Winkler2.\n",
       " .\n",
       " \"Les Canadiens sont bons\", dit la mairesse3 de Bobigny.\n",
       " .\n",
       " \"Les Indiens aussi sont bons\".\n",
       " .\n",
       " Jean Winkler4 dit, \"Les Canadiens sont bons\".\n",
       " .\n",
       " Jean Winkler5 dit \"Les Canadiens sont bons\".\n",
       " .\n",
       " Jean Winkler6 dit, que les Canadiens sont bons.\n",
       " .\n",
       " Jean Winkler7 dit que les Canadiens sont bons.\n",
       " .\n",
       " .\n",
       " .\n",
       " \"Les chinois mangent du riz.\", a-t-il dit.\n",
       " .\n",
       " \"Les Canadiens mangent du pain\", a-t-il dit.\n",
       " .\n",
       " Les Canadiens mangent du pain, a-t-il dit.\n",
       " .\n",
       " .\n",
       " .\n",
       " \"Les Canadiens mangent du pain\", selon Jean Winkler8.\n",
       " .\n",
       " Les Canadiens mangent du pain, selon Jean Winkler9.\n",
       " .\n",
       " Les Canadiens mangent du pain, d'après Jean Winkler10.\n",
       " .\n",
       " Selon Jean Winkler11, les Canadiens mangent du pain.\n",
       " .\n",
       " Selon Jean Winkler12, \"Les Canadiens mangent du pain\".\n",
       " .\n",
       " D'après Jean Winkler13, \"Les Canadiens mangent du pain\".\n",
       " .\n",
       " .\n",
       " .\n",
       " \"Les chinois mangent du riz.\n",
       " .\n",
       " Blah blah blah filler stuff.... Blah blah blah filler stuff....\n",
       " .\n",
       " Blah blah blah filler stuff....\n",
       " .\n",
       " Hao Chen dit que les chinois mangent du poisson aussi.\n",
       " .\n",
       " \"Les chinois boivent du thé.\""
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
